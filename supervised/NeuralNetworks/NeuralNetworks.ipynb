{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks are a subset of machine learning algorithms inspired by the structure and function of the human brain. They consist of interconnected layers of nodes (neurons) that process data by passing it through weights, biases, and activation functions.\n",
    "\n",
    "## Key Components\n",
    "1. **Neurons (Nodes)**: Basic computational units that receive input, apply a weight and bias, and pass it through an activation function.\n",
    "2. **Layers**:\n",
    "   - **Input Layer**: Receives the raw data.\n",
    "   - **Hidden Layers**: Perform computations, enabling the network to learn complex patterns.\n",
    "   - **Output Layer**: Produces the final result.\n",
    "3. **Weights and Biases**: Parameters adjusted during training to minimize error.\n",
    "4. **Activation Functions**: Introduce non-linearity, allowing the network to learn more complex mappings (e.g., ReLU, sigmoid, tanh).\n",
    "\n",
    "## Types of Neural Networks\n",
    "1. **Feedforward Neural Networks (FNN)**: Data flows in one direction, often used for tasks like classification and regression.\n",
    "2. **Convolutional Neural Networks (CNNs)**: Specialized for image data, using convolution layers to extract features.\n",
    "3. **Recurrent Neural Networks (RNNs)**: Designed for sequential data (e.g., time series, text), using loops to retain memory of previous inputs.\n",
    "4. **Generative Adversarial Networks (GANs)**: Consist of two networks (generator and discriminator) that compete to generate realistic data.\n",
    "5. **Transformers**: Modern architectures (e.g., GPT, BERT) for sequence processing, replacing RNNs in many applications.\n",
    "\n",
    "## Applications\n",
    "- **Manufacturing**: Predictive maintenance, quality control, demand forecasting.\n",
    "- **Healthcare**: Disease diagnosis, drug discovery.\n",
    "- **Finance**: Fraud detection, algorithmic trading.\n",
    "- **Natural Language Processing (NLP)**: Chatbots, translation, sentiment analysis.\n",
    "- **Image Processing**: Object detection, facial recognition.\n",
    "\n",
    "## Training a Neural Network\n",
    "1. **Forward Propagation**: Compute the output for a given input.\n",
    "2. **Loss Function**: Measure the difference between the predicted output and the actual target.\n",
    "3. **Backward Propagation**: Adjust weights and biases using gradient descent to minimize the loss.\n",
    "4. **Optimization**: Algorithms like stochastic gradient descent (SGD), Adam, or RMSprop refine the model.\n",
    "\n",
    "## Challenges\n",
    "- **Overfitting**: The network performs well on training data but poorly on unseen data.\n",
    "- **Data Requirements**: Neural networks often require large datasets.\n",
    "- **Computation**: Training deep networks can be resource-intensive.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying spam e-mails with neural networks\n",
    "\n",
    "A common use for binary classification is sorting spam e-mails from legitimate e-mails. I use Keras to build a binary classifier for e-mails, train it with a collection of e-mails labeled with 0s (for not spam) and 1s (for spam), and then run a few e-mails through it to see how well it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsSpam</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>key issues going forwarda year end reviews rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>congrats contratulations the execution the cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>key issues going forwardall under control set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>epmi files protest entergy transcoattached our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>california power please contact kristin walsh ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IsSpam                                               Text\n",
       "0       0  key issues going forwarda year end reviews rep...\n",
       "1       0  congrats contratulations the execution the cen...\n",
       "2       0   key issues going forwardall under control set...\n",
       "3       0  epmi files protest entergy transcoattached our...\n",
       "4       0  california power please contact kristin walsh ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('ham-spam.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how many rows the dataset contains and confirm that there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   IsSpam  1000 non-null   int64 \n",
      " 1   Text    1000 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate rows from the dataset and check for balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsSpam</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "      <td>key issues going forwarda year end reviews rep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>take the reinsbecomeyour employer substantial ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Text                                                               \n",
       "       count unique                                                top freq\n",
       "IsSpam                                                                     \n",
       "0        499    499  key issues going forwarda year end reviews rep...    1\n",
       "1        500    500  take the reinsbecomeyour employer substantial ...    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.groupby('IsSpam').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature column *x* containing the text in the \"Text\" column with stopwords removed, and a label column *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# First, download required NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # Use a simpler tokenization approach initially\n",
    "    text = text.lower().split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word.isalpha() and not word in stop_words]\n",
    "    return ' '.join(text)\n",
    "    \n",
    "x = df.apply(lambda row: remove_stop_words(row['Text']), axis=1)\n",
    "y = df['IsSpam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text and create padded sequences from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 20000\n",
    "max_length = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "x = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a neural network to identify spam\n",
    "\n",
    "Create a neural network containing an [`Embedding`](https://keras.io/api/layers/core_layers/embedding/) layer for converting sequences into arrays of word vectors and a [`Dense`](https://keras.io/api/layers/core_layers/dense/) layer for classifying arrays of word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           640000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               2048128   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,688,257\n",
      "Trainable params: 2,688,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(max_words, 32, input_length=max_length)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           640000    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16000)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               2048128   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,688,257\n",
      "Trainable params: 2,688,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding  # Import Embedding directly from keras.layers\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(max_words, 32, input_length=max_length)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 2s 32ms/step - loss: 0.6607 - accuracy: 0.6120 - val_loss: 0.7363 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 0.4455 - accuracy: 0.7071 - val_loss: 0.7671 - val_accuracy: 0.3900\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 1s 28ms/step - loss: 0.1447 - accuracy: 0.9725 - val_loss: 0.1923 - val_accuracy: 0.9650\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 0.0229 - accuracy: 0.9962 - val_loss: 0.1394 - val_accuracy: 0.9700\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 1s 30ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.3295 - val_accuracy: 0.8350\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x, y, validation_split=0.2, epochs=5, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart the training and validation accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "acc = hist.history['accuracy']\n",
    "val = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a convolutional neural network to identify spam\n",
    "\n",
    "Convolutional neural networks (CNNs) are primarily used to classify images, but they can be helpful for text classification, too. One advantage to a CNN is that it can recognize word patterns and factor them into its predictions rather than treat words independently. Let's train a CNN and see if it can achieve a higher validation accuracy than a conventional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 32)           640000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 494, 32)           7200      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 98, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 92, 32)            7200      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 654,433\n",
      "Trainable params: 654,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(max_words, 32, input_length=max_length)) \n",
    "model.add(Conv1D(32, 7, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(32, 7, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 2s 24ms/step - loss: 0.6546 - accuracy: 0.6245 - val_loss: 0.8970 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 0.5180 - accuracy: 0.6508 - val_loss: 0.7041 - val_accuracy: 0.6650\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 0.2261 - accuracy: 0.9700 - val_loss: 0.2829 - val_accuracy: 0.9600\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 0.0520 - accuracy: 0.9862 - val_loss: 0.1312 - val_accuracy: 0.9750\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 1s 25ms/step - loss: 0.0272 - accuracy: 0.9925 - val_loss: 0.1076 - val_accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x, y, validation_split=0.2, epochs=5, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart the training and validation accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model to classify e-mails\n",
    "\n",
    "Now let's see how the model classifies some sample e-mails. We'll start with a message that is not spam. The model's `predict` method predicts the probability that the input belongs to the positive class (spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 143ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3992149"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = remove_stop_words('Can you attend a code review on Tuesday? Need to make sure the logic is rock solid.')\n",
    "sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_length)\n",
    "model.predict(padded_sequence)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now test the model with a spam message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6705084"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = remove_stop_words('Why pay more for expensive meds when you can order them online and save $$$?')\n",
    "sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_length)\n",
    "model.predict(padded_sequence)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test with my last email from advisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "Email text: There is no group meeting today. Because of faculty recruiting,.. the end of this semester was a lot busier than expected. Since some of you will leave town soon, I wish you all happy holidays and all the best for the New Year via e-mail.  We will have a get together early next year.\n",
      "Probability of being spam: 14.33%\n",
      "Classification: NOT SPAM\n"
     ]
    }
   ],
   "source": [
    "# Test with a new email\n",
    "test_email = \"There is no group meeting today. Because of faculty recruiting,.. the end of this semester was a lot busier than expected. Since some of you will leave town soon, I wish you all happy holidays and all the best for the New Year via e-mail.  We will have a get together early next year.\"\n",
    "cleaned_text = remove_stop_words(test_email)\n",
    "sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_length)\n",
    "prediction = model.predict(padded_sequence)[0][0]\n",
    "\n",
    "print(f\"Email text: {test_email}\")\n",
    "print(f\"Probability of being spam: {prediction:.2%}\")\n",
    "print(f\"Classification: {'SPAM' if prediction > 0.5 else 'NOT SPAM'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now we have a working spam filter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
