{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN: Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "DBSCAN is an unsupervised machine learning algorithm used for clustering tasks. It is particularly effective for identifying clusters of arbitrary shape and handling noise (outliers).\n",
    "\n",
    "## Key Concepts\n",
    "- **Core Points**: Points with at least a minimum number of neighbors (`minPts`) within a specified radius (`eps`).\n",
    "- **Border Points**: Points within the `eps` radius of a core point but with fewer than `minPts` neighbors.\n",
    "- **Noise Points**: Points that are neither core points nor border points.\n",
    "\n",
    "## Parameters\n",
    "1. **`eps`**: The radius of the neighborhood around a point.\n",
    "2. **`minPts`**: The minimum number of points required to form a dense region (a cluster).\n",
    "\n",
    "## Algorithm Steps\n",
    "1. Start with an unvisited point.\n",
    "2. If the point is a core point (has at least `minPts` points within `eps`):\n",
    "   - Create a new cluster.\n",
    "   - Add all directly connected points (within `eps`) to this cluster.\n",
    "   - Recursively expand the cluster by visiting neighbors of neighbors.\n",
    "3. If the point is not a core point and not connected to any cluster, label it as noise.\n",
    "4. Repeat until all points are visited.\n",
    "\n",
    "## Advantages\n",
    "- **No need to specify the number of clusters**: The algorithm determines clusters based on data distribution.\n",
    "- **Handles arbitrary shapes**: Unlike k-means, DBSCAN identifies clusters of any shape.\n",
    "- **Noise resilience**: Explicitly labels noise points for outlier detection.\n",
    "\n",
    "## Limitations\n",
    "- **Parameter sensitivity**: Proper selection of `eps` and `minPts` is crucial for good results.\n",
    "- **Scalability**: DBSCAN can be computationally intensive for large datasets.\n",
    "- **Difficulty with varying densities**: Struggles with clusters that have widely varying densities.\n",
    "\n",
    "## Applications\n",
    "- Anomaly detection (e.g., fraud detection, fault detection).\n",
    "- Spatial data analysis (e.g., geographic clustering).\n",
    "- Customer segmentation.\n",
    "- Community detection in social networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing function\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Raw consumer complaints dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame: Processed features for clustering\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Select relevant features for clustering\n",
    "    categorical_features = [\n",
    "        'product', 'sub_product', 'issue', 'sub_issue', \n",
    "        'state', 'submitted_via', 'company_response_to_consumer',\n",
    "        'timely_response', 'consumer_disputed?'\n",
    "    ]\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_features:\n",
    "        # Handle missing values first\n",
    "        data[col] = data[col].fillna('Unknown')\n",
    "        data[col] = le.fit_transform(data[col].astype(str))\n",
    "    \n",
    "    # Select features for clustering\n",
    "    features_for_clustering = [\n",
    "        'product', 'sub_product', 'issue', 'sub_issue', \n",
    "        'state', 'submitted_via', 'company_response_to_consumer',\n",
    "        'timely_response', 'consumer_disputed?'\n",
    "    ]\n",
    "    \n",
    "    # Prepare the feature matrix\n",
    "    X = data[features_for_clustering]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, X.columns\n",
    "\n",
    "def perform_dbscan_clustering(X_scaled, eps_range=None, min_samples_range=None):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering with parameter tuning\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_scaled : array-like\n",
    "        Scaled feature matrix\n",
    "    eps_range : list, optional\n",
    "        Range of epsilon values to test\n",
    "    min_samples_range : list, optional\n",
    "        Range of min_samples values to test\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (best DBSCAN model, clustering results)\n",
    "    \"\"\"\n",
    "    # Default parameter ranges if not provided\n",
    "    if eps_range is None:\n",
    "        eps_range = np.linspace(0.1, 2, 20)\n",
    "    if min_samples_range is None:\n",
    "        min_samples_range = range(2, 10)\n",
    "    \n",
    "    # Parameter tuning\n",
    "    best_silhouette = -1\n",
    "    best_eps = 0\n",
    "    best_min_samples = 0\n",
    "    best_labels = None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for eps in eps_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            # Perform DBSCAN clustering\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(X_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_noise = list(labels).count(-1)\n",
    "            \n",
    "            # Calculate silhouette score (excluding noise points)\n",
    "            if n_clusters > 1:\n",
    "                try:\n",
    "                    silhouette_avg = silhouette_score(\n",
    "                        X_scaled[labels != -1], \n",
    "                        labels[labels != -1]\n",
    "                    )\n",
    "                except:\n",
    "                    silhouette_avg = -1\n",
    "            else:\n",
    "                silhouette_avg = -1\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'n_clusters': n_clusters,\n",
    "                'n_noise': n_noise,\n",
    "                'silhouette_score': silhouette_avg\n",
    "            })\n",
    "            \n",
    "            # Update best parameters\n",
    "            if silhouette_avg > best_silhouette:\n",
    "                best_silhouette = silhouette_avg\n",
    "                best_eps = eps\n",
    "                best_min_samples = min_samples\n",
    "                best_labels = labels\n",
    "    \n",
    "    # Perform final DBSCAN with best parameters\n",
    "    best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "    best_dbscan.fit(X_scaled)\n",
    "    \n",
    "    return best_dbscan, best_labels, results, best_eps, best_min_samples\n",
    "\n",
    "def visualize_clustering_results(X_scaled, labels, feature_names):\n",
    "    \"\"\"\n",
    "    Visualize DBSCAN clustering results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_scaled : array-like\n",
    "        Scaled feature matrix\n",
    "    labels : array-like\n",
    "        Cluster labels\n",
    "    feature_names : array-like\n",
    "        Names of features used for clustering\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. PCA Visualization\n",
    "    plt.subplot(221)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Scatter plot with different colors for each cluster\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise\n",
    "            col = [0, 0, 0, 1]\n",
    "        \n",
    "        class_member_mask = (labels == k)\n",
    "        xy = X_pca[class_member_mask]\n",
    "        plt.scatter(xy[:, 0], xy[:, 1], color=col, \n",
    "                    label=f'Cluster {k}' if k != -1 else 'Noise',\n",
    "                    alpha=0.7)\n",
    "    \n",
    "    plt.title('PCA Visualization of Clusters')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 2. Cluster Size Distribution\n",
    "    plt.subplot(222)\n",
    "    cluster_sizes = pd.Series(labels).value_counts()\n",
    "    cluster_sizes.plot(kind='bar')\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.xlabel('Cluster Label')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    \n",
    "    # 3. Hyperparameter Tuning Results\n",
    "    plt.subplot(223)\n",
    "    tuning_results = pd.DataFrame(results)\n",
    "    plt.scatter(\n",
    "        tuning_results['eps'], \n",
    "        tuning_results['silhouette_score'], \n",
    "        c=tuning_results['n_clusters'], \n",
    "        cmap='viridis'\n",
    "    )\n",
    "    plt.colorbar(label='Number of Clusters')\n",
    "    plt.title('Hyperparameter Tuning')\n",
    "    plt.xlabel('Epsilon')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    \n",
    "    # 4. Feature Importance in Clustering\n",
    "    plt.subplot(224)\n",
    "    # Compute feature importance using variance between clusters\n",
    "    feature_importance = []\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        # Calculate variance of feature across different clusters\n",
    "        feature_var = [\n",
    "            X_scaled[labels == cluster, i].var() \n",
    "            for cluster in set(labels) if cluster != -1\n",
    "        ]\n",
    "        feature_importance.append(np.mean(feature_var))\n",
    "    \n",
    "    plt.barh(feature_names, feature_importance)\n",
    "    plt.title('Feature Importance in Clustering')\n",
    "    plt.xlabel('Variance Contribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print additional clustering statistics\n",
    "    print(\"\\nClustering Statistics:\")\n",
    "    print(f\"Number of Clusters: {len(set(labels)) - (1 if -1 in labels else 0)}\")\n",
    "    print(f\"Number of Noise Points: {list(labels).count(-1)}\")\n",
    "    print(f\"Best Epsilon: {best_eps}\")\n",
    "    print(f\"Best Minimum Samples: {best_min_samples}\")\n",
    "\n",
    "def main():\n",
    "    # Load the data\n",
    "    df = pd.read_csv('C:/Users/NANAYAW/OneDrive/Documents/GitHub/FinalProject/consumer_complaints.csv')\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_scaled, feature_names = preprocess_data(df)\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "    dbscan, labels, results, best_eps, best_min_samples = perform_dbscan_clustering(X_scaled)\n",
    "    \n",
    "    # Visualize clustering results\n",
    "    visualize_clustering_results(X_scaled, labels, feature_names)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
